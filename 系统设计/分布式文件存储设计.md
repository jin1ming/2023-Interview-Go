# 分布式文件存储系统设计

## 1. 需求分析 (Requirements)

### 1.1 功能需求 (Functional)
- **文件上传/写入**：支持大文件（GB/TB级别）写入，支持追加写入（Append）。
- **文件下载/读取**：支持按偏移量读取。
- **文件删除**：支持删除文件。
- **目录结构**：支持层级目录（类似 Linux 文件系统）。

### 1.2 非功能需求 (Non-Functional)
- **高可靠性 (Durability)**：数据不丢失，能够容忍磁盘损坏或节点宕机。
- **高可用性 (Availability)**：系统需 7*24 小时可用，Master 节点不能成为单点故障。
- **高扩展性 (Scalability)**：支持通过增加普通机器来线性扩展存储容量和吞吐量。
- **高性能**：针对大文件优化高吞吐量，针对小文件优化低延迟。

---

## 2. 总体架构 (High-Level Architecture)

采用经典的 **Master-Slave (主从)** 架构，类似于 GFS/HDFS。

### 2.1 组件角色
1.  **Client (客户端)**
    - 提供文件操作 API (open, read, write, close)。
    - 与 Master 交互获取元数据（文件在哪里）。
    - 与 ChunkServer 交互进行数据读写。
    - **缓存元数据**：减少与 Master 的交互频率。

2.  **Master (元数据节点 / NameNode)**
    - **存储元数据**：
        - **文件树**：文件名 -> Chunk 列表的映射。
        - **Chunk 位置**：Chunk -> ChunkServer 列表的映射（通常主要在启动时由 ChunkServer 上报，内存维护）。
    - **管理操作**：处理命名空间操作（创建/删除文件）、Chunk 的副本管理（复制、再平衡）、垃圾回收。
    - **单点问题**：通常是单点，通过 HA 方案（如 Raft/ZooKeeper + Standby）解决。

3.  **ChunkServer (数据节点 / DataNode)**
    - **存储数据**：将文件切分为固定大小的块（Chunk/Block），存储在本地 Linux 文件系统中。
    - **读写处理**：处理 Client 的读写请求。
    - **心跳汇报**：定期向 Master 汇报存活状态和持有的 Chunk 列表。

---

## 3. 关键设计决策 (Key Design Decisions)

### 3.1 数据分块 (Chunk Size)
- **策略**：将大文件切分为固定大小的 Chunk（例如 64MB 或 128MB）。
- **为什么选大块？**
    - 减少 Master 元数据数量（内存是 Master 的瓶颈）。
    - 减少 Client 与 Master 的交互。
    - 保持 Client 与 ChunkServer 的长连接，提高吞吐量。
- **弊端**：小文件也会占用一个 Chunk（逻辑上），如果是大量小文件，元数据压力依然大（需专门优化）。

### 3.2 元数据管理 (Metadata)
- **存储方式**：Master 将元数据全量加载在**内存**中，保证极高的响应速度。
- **持久化**：
    - **Operation Log (EditLog)**：记录每一步修改操作（WAL），用于恢复。
    - **Checkpoint (FsImage)**：定期将内存状态快照到磁盘，加速重启恢复。

### 3.3 副本策略 (Replication)
- **默认副本数**：3。
- **放置策略 (Rack Awareness)**：
    - 副本 1：写入客户端所在的 DataNode（或随机）。
    - 副本 2：不同机架的 DataNode（防机架断电）。
    - 副本 3：与副本 2 同机架的另一台 DataNode。
- **一致性**：
    - 强一致性 vs 最终一致性。GFS 选择放宽一致性以换取性能，HDFS 保证写完后立即可读。

---

## 4. 核心流程 (Core Workflows)

### 4.1 文件读取流程 (Read)
1.  **Client -> Master**：发送文件名和读取偏移量 (Offset)。
2.  **Master -> Client**：根据 Offset 计算出 Chunk Index，返回该 Chunk 的 Handle (ID) 和副本位置列表 (Replica Locations)。
3.  **Client 缓存**：Client 缓存该元数据。
4.  **Client -> ChunkServer**：选择最近的一个副本（如本机或同机架），发送读请求。
5.  **ChunkServer -> Client**：返回数据。

### 4.2 文件写入流程 (Write)
1.  **Client -> Master**：请求创建文件或分配新的 Chunk。
2.  **Master**：分配 Chunk ID，选择 3 个 ChunkServer 存放副本，返回列表（Primary + Secondaries）。
3.  **Client -> ChunkServers**：
    - **数据流 (Pipeline)**：Client 将数据推送到第 1 个 DataNode，第 1 个推给第 2 个，以此类推。充分利用带宽。
    - **控制流**：数据传输完毕后，Client 向 Primary DataNode 发送“确认写入”指令。
4.  **Primary Commit**：Primary 安排写入顺序，本地写入成功后，通知所有 Secondary 写入。
5.  **响应**：所有副本写入成功，Primary 响应 Client 成功；否则响应失败，Client 重试。

---

## 5. 高级特性与优化 (Advanced)

### 5.1 Master 高可用 (HA)
- **热备 (Hot Standby)**：使用 ZooKeeper 选主。Active Master 挂掉后，Standby 立即接管。
- **元数据同步**：Active Master 将 EditLog 写入共享存储（如 Quorum Journal Manager / NFS），Standby 实时读取并应用。

### 5.2 应对海量小文件 (Small Files Problem)
- **问题**：由于 Master 内存限制，大量小文件（KB级）会耗尽 Master 内存。
- **解决方案**：
    - **合并存储 (Hadoop HAR / SequenceFile)**：将多个小文件打包成一个大文件。
    - **Facebook Haystack / Taobao TFS 思路**：
        - 不在 Master 存具体文件的 Chunk 映射。
        - Master 只维护“逻辑卷 (Volume)”的映射。
        - 文件路径内包含 `VolumeID + Offset`，Client 直接去 Volume 对应的 Server 读取。

### 5.3 纠删码 (Erasure Coding, EC)
- **背景**：3 副本策略存储利用率仅 33%。冷数据成本高。
- **方案**：使用 RS (Reed-Solomon) 编码。例如 6 个数据块 + 3 个校验块。
- **效果**：存储利用率提升到 66% 甚至更高，且容错能力不降。缺点是恢复数据和写入计算开销大，适合**冷数据**。

### 5.4 数据完整性
- **Checksum**：每个 Chunk 划分为 64KB 的 Block，每个 Block 存储 32bit Checksum。
- **读取验证**：读取时校验 Checksum，发现损坏则从其他副本读取，并通知 Master 修复。

---

## 6. 面试重点与常见追问 (Interview Deep Dive)

作为面试者，在设计完上述架构后，需要准备好回答以下深层问题，这些往往是区分 Senior/Staff 级别的关键。

### 6.1 为什么选择单 Master 架构？这不会是瓶颈吗？
-   **回答逻辑**：
    1.  **简单性**：单 Master 大大简化了元数据的一致性管理（不需要复杂的分布式共识算法来维护文件树）。
    2.  **瓶颈分析**：
        -   **内存**：假设每个文件元数据占 100 Bytes，64GB 内存可以存约 6 亿个文件。对于大文件存储，这个数量级通常够用。
        -   **CPU/IO**：Client 只有在 Open/Create 时访问 Master，数据读写直接走 ChunkServer，Master 的 QPS 压力并没有想象中大。
    3.  **扩展方案**：如果真的遇到瓶颈，可以提出 **HDFS Federation**（多个 Master 管理不同的目录命名空间，但共享 ChunkServer）作为扩展方案。

### 6.2 写入过程中，如果一个副本写失败了怎么办？
-   **GFS 模式（弱一致性）**：如果某个副本失败，Master 认为写入失败，Client 需要重试。重试可能导致部分副本有数据，部分没有（垃圾数据），后续通过垃圾回收清理。这会导致文件出现“重复记录”或“不一致”，由应用层去重。
-   **HDFS 模式（强一致性）**：
    -   **Pipeline 恢复**：如果 Pipeline 中某个 DataNode 挂了，Pipeline 会关闭，将 Ack 队列中的包写回数据队列。
    -   **剔除坏节点**：从 Pipeline 中剔除坏掉的节点，将剩下的数据写入剩余的健康节点。
    -   **上报 Master**：Master 后续发现副本数不足（<3），会在后台异步触发复制，补齐副本。

### 6.3 为什么数据传输要用 Pipeline（流水线），而不是 Client 并发发给 3 个节点？
-   **利用带宽**：Client 通常在机房外部或单机带宽有限。如果 Client 同时发 3 份数据，它的上行带宽会成为瓶颈 (Bandwidth / 3)。
-   **线性拓扑**：使用 Pipeline，Client 只需要以全速发一份数据给第一个 DataNode。DataNode 往往位于内网，带宽充裕，由它们接力传输，能最大化利用整个集群的总带宽。

### 6.4 如何检测 **Bit Rot**（静默数据损坏）？
-   **场景**：硬盘随时间推移，磁性介质可能发生翻转，导致读取数据错误但硬盘没有报错。
-   **方案**：
    -   **Checksum**：写入时计算 Checksum（如 CRC32），随数据一起存。
    -   **读取校验**：读取时重新计算并比对。
    -   **后台扫描 (DataScanner)**：DataNode 后台线程定期扫描所有 Block，验证 Checksum，发现损坏自动向 Master 汇报并从其他副本恢复。

### 6.5 怎么处理“读”的高并发（热点文件）？
-   **客户端缓存**：虽然不能缓存大文件数据，但可以缓存元数据。
-   **增加副本**：Master 发现某个 Chunk 读取请求过高，可以动态在该区域增加副本数（Replication Factor）。
-   **CDN / 专用缓存层**：如果是静态文件，前面挡一层 Nginx 或 CDN。
