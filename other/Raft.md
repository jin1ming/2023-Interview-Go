#### Raft算法

> **Raft算法是现在分布式系统开发首选的共识算法。**
>
> **从本质上说，Raft 算法是通过一切以领导者为准的方式，实现一系列值的共识和各节点日志的一致。**

##### 成员身份/服务器节点状态

- `Raft`算法支持领导者（`Leader`）、跟随者（`Follower`）和候选人（`Candidate`）`3`种状态。
- 在任何时候，每一个服务器节点都处于这`3`个状态中的`1`个。

- **跟随者**：

  - 就相当于普通群众，默默地接收和处理来自领导者的消息。
  - 当等待领导者心跳信息超时的时候，就主动站出来，推荐自己当候选人。

- **候选人**：
- 候选人将向其他节点发送请求投票（`RequestVote`）`RPC` 消息，通知其他节点来投票。
  - 如果赢得了大多数选票，就晋升当领导者。
  
- **领导者**：

  - 蛮不讲理的霸道总裁，一切以我为准，平常的主要工作内容就是`3`部分：**处理写请求**、**管理日志复制**和**不断地发送心跳信息**。
  - 心跳信息：通知其他节点“我是领导者，我还活着，你们现在不要发起新的选举，找个新领导者来替代我。”


##### 选举领导者

- 首先，在初始状态下，集群中所有的节点都是跟随者的状态。

- `Raft`算法实现了随机超时时间的特性。

- 也就是说，每个节点等待领导者节点心跳信息的超时时间间隔是随机的。

  - 通过上面的图片你可以看到，集群中没有领导者，而节点`A`的等待超时时间最小（`150ms`），它会最先因为没有等到领导者的心跳信息，发生超时。

- 这个时候，节点`A`就增加自己的任期编号，并推举自己为候选人，先给自己投上一张选票。

- 然后向其他节点发送请求投票`RPC`消息，请它们选举自己为领导者。

- 如果其他节点接收到候选人`A`的请求投票`RPC`消息，在编号为`1`的这个任期内，也还没有进行过投票，那么它就把选票投给节点`A`，并增加自己的任期编号。

- 如果候选人在选举超时时间内赢得了大多数的选票，那么它就会成为本届任期内新的领导者。

- 节点`A`当选领导者后，它会周期性地发送心跳信息，通知其他服务器我是领导者，阻止跟随者发起新的选举、篡权。


##### 选举过程的四个问题

###### 1. 节点间如何通讯？

- 在`Raft`算法中，服务器节点间的沟通联络采用的是远程过程调用（`RPC`）。
- 在领导者选举过程中，需要用到这样两类的`RPC`：
  1. **请求投票（`Request Vote`）`RPC`**：
     - 是由候选人在选举期间发起的，用于通知各节点进行投票。
  2. **日志服务（`Append Entries`）`RPC`**：
     - 是由领导者发起的，用于复制日志和提供心跳信息。
- 注意：**日志复制`RPC`只能由领导者发起，这是实现强领导者模型的关键之一**。

###### 2. 什么是任期？

- 我们知道，议会选举中的领导者是有任期的，领导者任命到期后，要重新开会再次选举。
- `Raft`算法中的领导者也是有任期的，每个任期由单调递增的数字（任期编号）标识。
  - 比如节点`A` 的任期编号是`1`。
- 任期编号是随着选举的举行而变化的，这是在说下面几点：
  1. 跟随者在等待领导者心跳信息超时后，推举自己为候选人时，会增加自己的任期号。
  2. 如果一个服务器节点，发现自己的任期编号比其他节点小，那么它会更新自己的编号到较大的编号值。
- **与现实议会选举中的领导者的任期不同，Raft 算法中的任期不只是时间段，而且任期编号的大小，会影响领导者选举和请求的处理。**
  1. 在`Raft`算法中约定，如果一个候选人或者领导者，发现自己的任期编号比其他节点小，那么它会立即恢复成跟随者状态。
  2. `Raft`算法还约定，如果一个节点接收到一个包含较小的任期编号值的请求，那么它会直接拒绝这个请求。

###### 3. 选举有哪些规则？

- 在议会选举中，比成员的身份、领导者的任期还要重要的就是选举的规则。

- 比如一人一票、弹劾制度等。

- “无规矩不成方圆”，在`Raft`算法中，也约定了选举规则，主要有这样几点：

  1. 领导者周期性地向所有跟随者发送心跳消息（即不包含日志项的日志复制`RPC`消息）。通知大家我是领导者，阻止跟随者发起新的选举。

  2. 如果在指定时间内，跟随者没有接收到来自领导者的消息，那么它就认为当前没有领导者，推举自己为候选人，发起领导者选举。

  3. 在一次选举中，赢得大多数选票的候选人，将晋升为领导者。

  4. 在一个任期内，领导者一直都会是领导者，直到它自身出现问题（比如宕机），或者因为网络延迟，其他节点发起一轮新的选举。

  5. 在一次选举中，每一个服务器节点最多会对一个任期编号投出一张选票，并且按照“先来先服务”的原则进行投票。

  6. 日志完整性高的跟随者（也就是最后一条日志项对应的任期编号值更大，索引号更大），拒绝投票给日志完整性低的候选人。

     - 比如节点`B`的任期编号为`3`，节点`C`的任期编号是`4`。
     - 节点`B`的最后一条日志项对应的任期编号为`3`，而节点`C`为`2`，那么当节点 C 请求节点 `B` 投票给自己时，节点`B`将拒绝投票。

     ![image-20210328141622273](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/image-20210328141622273.png)

注意：

- **选举是跟随者发起的，推举自己为候选人。**
- **大多数选票是指集群成员半数以上的选票。**
- **大多数选票规则的目标，是为了保证在一个给定的任期内最多只有一个领导者。**
- 在选举中，除了选举规则外，我们还需要避免一些会导致选举失败的情况，**比如同一任期内，多个候选人同时发起选举，导致选票被瓜分，选举失败。那么在`Raft`算法中，如何避免这个问题呢？答案就是随机超时时间。**

###### Leader选举总结

- `Raft`算法和兰伯特的`Multi-Paxos`不同之处，主要有 2 点：
  1. 首先，在`Raft`中，不是所有节点都能当选领导者，只有日志较完整的节点（也就是日志完整度不比半数节点低的节点），才能当选领导者。
  2. 其次，在`Raft`中，日志必须是连续的。
- `Raft`算法通过任期、领导者心跳消息、随机选举超时时间、先来先服务的投票原则、大多数选票原则等，保证了一个任期只有一位领导，也极大地减少了选举失败的情况。
- 本质上，`Raft`算法以领导者为中心，选举出的领导者，以“一切以我为准”的方式，达成值的共识，和实现各节点日志的一致。

##### 日志复制

> **选完领导者之后，领导者需要处理来自客户端的写请求，并通过日志复制实现各节点日志的一致。**
>
> **在`Raft`算法中，副本数据是以日志的形式存在的，领导者接收到来自客户端写请求后，处理写请求的过程就是一个复制和应用（`Apply`）日志项到状态机的过程。**

###### 1. 如何理解日志

- 副本数据是以日志的形式存在的，日志是由日志项组成的。

- 日志项是一种数据格式，它主要包含用户指定的数据，也就是指令（`Command`），还包含一些附加信息，比如索引值（`Log index`）、任期编号（`Term`）。

  ![image-20210328142136861](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/image-20210328142136861.png)

  - 指令：一条由客户端请求指定的、状态机需要执行的指令。可以将指令理解成客户端指定的数据。
  - 索引值：日志项对应的整数索引值。它其实就是用来标识日志项的，是一个连续的、单调递增的整数号码。
  - 任期编号：创建这条日志项的领导者的任期编号。

- 一届领导者任期，往往有多条日志项。而且日志项的索引值是连续的。

###### 2. 如何复制日志

- 可以把`Raft`的日志复制理解成一个优化后的二阶段提交（将二阶段优化成了一阶段），减少了一半的往返消息，也就是降低了一半的消息延迟。

- 具体过程：

  1. 首先，领导者进入第一阶段，通过日志复制（`AppendEntries`）`RPC`消息，将日志项复制到集群其他节点上。

  2. 接着，如果领导者接收到大多数的“复制成功”响应后，它将日志项应用到它的状态机，并返回成功给客户端。

     - 如果领导者没有接收到大多数的“复制成功”响应，那么就返回错误给客户端。

     - 领导者将日志项应用到它的状态机，怎么没通知跟随者应用日志项呢？

       - 这是`Raft`中的一个优化，领导者不直接发送消息通知其他节点应用指定日志项。
       - 因为领导者的日志复制`RPC`消息或心跳消息，包含了当前最大的，将会被提交（`Commit`）的日志项索引值。
       - 所以通过日志复制`RPC`消息或心跳消息，跟随者就可以知道领导者的日志提交位置信息。
       - 因此，当其他节点接受领导者的心跳消息，或者新的日志复制 RPC 消息后，就会将这条日志项应用到它的状态机。
       - 而这个优化，降低了处理客户端请求的延迟，将二阶段提交优化为了一段提交，降低了一半的消息延迟。

       ![image-20210328142307011](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/image-20210328142307011.png)

- **日志复制过程总结**：

  1. 接收到客户端请求后，领导者基于客户端请求中的指令，创建一个新日志项，并附加到本地日志中。
  2. 领导者通过日志复制`RPC`，将新的日志项复制到其他的服务器。
  3. 当领导者将日志项，成功复制到大多数的服务器上的时候，领导者会将这条日志项应用到它的状态机中。
  4. 领导者将执行的结果返回给客户端。
  5. 当跟随者接收到心跳信息，或者新的日志复制`RPC`消息后，如果跟随者发现领导者已经提交了某条日志项，而它还没应用，那么跟随者就将这条日志项应用到本地的状态机中。

  不过，这是一个理想状态下的日志复制过程。在实际环境中，复制日志的时候，你可能会遇到进程崩溃、服务器宕机等问题，这些问题会导致日志不一致。那么在这种情况下，Raft 算法是如何处理不一致日志，实现日志的一致的呢？

###### 3. 如何实现日志的一致

- 在`Raft`算法中，领导者通过强制跟随者直接复制自己的日志项，处理不一致日志。

- 也就是说，`Raft`是通过以领导者的日志为准，来实现各节点日志的一致的。

- 具体有`2`个步骤：

  1. 首先，领导者通过日志复制`RPC`的一致性检查，找到跟随者节点上，与自己相同日志项的最大索引值。
     - 也就是说，这个索引值之前的日志，领导者和跟随者是一致的，之后的日志是不一致的了。
  2. 然后，领导者强制跟随者更新覆盖的不一致日志项，实现日志的一致。

- **详细过程**：

  - 为了方便演示，引入`2`个新变量：

    - `PrevLogEntry`：表示当前要复制的日志项，前面一条日志项的索引值。

      - 比如在图中，如果领导者将索引值为`8`的日志项发送给跟随者，那么此时 `PrevLogEntry`值为`7`。

    - `PrevLogTerm`：表示当前要复制的日志项，前面一条日志项的任期编号。

      - 比如在图中，如果领导者将索引值为`8`的日志项发送给跟随者，那么此时 `PrevLogTerm`值为`4`。

      ![image-20210328142447298](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/image-20210328142447298.png)

    1. 领导者通过日志复制`RPC`消息，发送当前最新日志项到跟随者。
       - 为了演示方便，假设当前需要复制的日志项是最新的。
       - 这个消息的`PrevLogEntry`值为`7`，`PrevLogTerm`值为`4`。
    2. 如果跟随者在它的日志中，找不到与`PrevLogEntry`值为`7`、`PrevLogTerm`值为`4`的日志项，也就是说它的日志和领导者的不一致了，那么跟随者就会拒绝接收新的日志项，并返回失败信息给领导者。
    3. 这时，领导者会递减要复制的日志项的索引值，并发送新的日志项到跟随者。
       - 这个消息的`PrevLogEntry`值为`6`，`PrevLogTerm`值为`3`。
    4. 如果跟随者在它的日志中，找到了`PrevLogEntry`值为`6`、`PrevLogTerm`值为`3`的日志项，那么日志复制`RPC`返回成功。
       - 这样，领导者就知道在`PrevLogEntry`值为`6`、`PrevLogTerm`值为`3`的位置，跟随者的日志项与自己相同。
    5. 领导者通过日志复制`RPC`，复制并更新覆盖该索引值之后的日志项（也就是不一致的日志项），最终实现了集群各节点日志的一致。

  > - **从上面步骤中可以看到，领导者通过日志复制`RPC`一致性检查，找到跟随者节点上与自己相同日志项的最大索引值，然后复制并更新覆盖该索引值之后的日志项，实现了各节点日志的一致。**
  > - **需要注意的是，跟随者中的不一致日志项会被领导者的日志覆盖，而且领导者从来不会覆盖或者删除自己的日志。**

###### 4. 日志复制总结

- 在`Raft`中，副本数据是以日志的形式存在的，其中日志项中的指令表示用户指定的数据。
- 兰伯特的`Muliti-Paxos`不要求日志是连续的，但在`Raft`中日志必须是连续的。
- 而在`Raft`中，日志不仅是数据的载体，日志的完整性还影响领导者选举的结果。
- 也就是说，日志完整性最高的节点才能当选领导者。
- `Raft`是通过以领导者的日志为准，来实现日志的一致的。

可以发现，值的共识和日志的一致都是由领导者决定的，领导者的唯一性很重要。

那如果需要对集群进行扩容或者缩容，比如将`3`节点集群扩容为`5`节点集群，这时候是可能同时出现两个领导者的。

这时就需要解决这个问题。（成员变更）

##### 成员变更

- 在日常工作中，你可能会遇到服务器故障的情况，这时你就需要替换集群中的服务器。
- 如果遇到需要改变数据副本数的情况，则需要增加或移除集群中的服务器。
- 总的来说，在日常工作中，集群中的服务器数量是会发生变化的。

讲到这儿，也许你会问：“`Raft`是共识算法，对集群成员进行变更时（比如增加`2`台服务器），会不会因为集群分裂，出现`2`个领导者呢？”

- 的确会出现这个问题，因为`Raft`的领导者选举，建立在“大多数”的基础之上，那么当成员变更时，集群成员发生了变化，就可能同时存在新旧配置的`2`个“大多数”，出现`2`个领导者，破坏了 `Raft`集群的领导者唯一性，影响了集群的运行。
- 关于成员变更，不仅是`Raft`算法中比较难理解的一部分，非常重要，也是`Raft`算法中唯一被优化和改进的部分。
  - 比如，最初实现成员变更的是联合共识（`Joint Consensus`），但这个方法实现起来难。
  - 后来`Raft`的作者就提出了一种改进后的方法，单节点变更（`single-server changes`）。

先介绍一下“配置”（`Configuration`）这个词儿：

- 它就是在说集群是哪些节点组成的，是集群各节点地址信息的集合。
  - 比如节点`A`、`B`、`C`组成的集群，那么集群的配置就是`[A, B, C]`集合。

###### 成员变更的问题

- 在集群中进行成员变更的最大风险是，可能会同时出现`2`个领导者。

  - 比如在进行成员变更时，节点`A`、`B`和`C`之间发生了分区错误，节点`A`、`B`组成旧配置中的“大多数”，也就是变更前的`3`节点集群中的“大多数”，那么这时的领导者（节点`A`）依旧是领导者。

  - 另一方面，节点`C`和新节点`D`、`E`组成了新配置的“大多数”，也就是变更后的`5`节点集群中的“大多数”，它们可能会选举出新的领导者（比如节点`C`）。

  - 那么这时，就出现了同时存在`2`个领导者的情况。

    ![image-20210328143303112](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/image-20210328143303112.png)

- 如果出现了`2`个领导者，那么就违背了“领导者的唯一性”的原则，进而影响到集群的稳定运行。

- 你要如何解决这个问题呢？也许有的同学想到了一个解决方法——**固定配置**。

  - 因为我们在启动集群时，配置是固定的，不存在成员变更，在这种情况下，`Raft`的领导者选举能保证只有一个领导者。
  - 也就是说，这时不会出现多个领导者的问题，那我可以先将集群关闭再启动新集群啊。
  - 也就是先把节点`A`、`B`、`C`组成的集群关闭，然后再启动节点`A`、`B`、`C`、`D`、`E`组成的新集群。
  - **这个方法不可行。 **
    - 为什么呢？因为你每次变更都要重启集群，意味着在集群变更期间服务不可用。
    - 肯定不行啊，太影响用户体验了。想象一下，你正在玩王者荣耀，时不时弹出一个对话框通知你：系统升级，游戏暂停`3`分钟。这体验糟糕不糟糕？
  - 既然这种方法影响用户体验，根本行不通，那**到底怎样解决成员变更的问题呢？**
    - **最常用的方法就是单节点变更。**

###### 如何通过单节点变更解决成员变更的问题？

> **单节点变更，就是通过一次变更一个节点实现成员变更。如果需要变更多个节点，那你需要执行多次单节点变更。**

- 比如将`3`节点集群扩容为`5`节点集群，这时你需要执行`2`次单节点变更，先将`3`节点集群变更为 `4`节点集群，然后再将`4`节点集群变更为`5`节点集群，就像下图的样子：

  ![image-20210328143546517](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/image-20210328143546517.png)

- **这样一来，我们就通过一次变更一个节点的方式，完成了成员变更，保证了集群中始终只有一个领导者，而且集群也在稳定运行，持续提供服务。**

- **大多数情况下，不存在新旧配置两个“大多数”：**

  - 我想说的是，**在正常情况下，不管旧的集群配置是怎么组成的，旧配置的“大多数”和新配置的“大多数”都会有一个节点是重叠的。 **
    - 也就是说，不会同时存在旧配置和新配置`2`个“大多数”：

  ![image-20210328143838487](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/image-20210328143838487.png)

  ![image-20210328143850157](https://raw.githubusercontent.com/MachineGunLin/markdown_pics/master/img/image-20210328143850157.png)

  - 从上图中你可以看到，不管集群是偶数节点，还是奇数节点，不管是增加节点，还是移除节点，新旧配置的“大多数”都会存在重叠（图中的橙色节点）。
  - 需要你注意的是，在分区错误、节点故障等情况下，如果我们并发执行单节点变更，那么就可能出现一次单节点变更尚未完成，新的单节点变更又在执行，导致集群出现`2`个领导者的情况。
  - 如果你遇到这种情况，可以在领导者启动时，创建一个`NO_OP`日志项（也就是空日志项），只有当领导者将`NO_OP`日志项应用后，再执行成员变更请求。

###### 成员变更总结

1. 成员变更的问题，主要在于进行成员变更时，可能存在新旧配置的`2`个“大多数”，导致集群中同时出现两个领导者，破坏了`Raft`的领导者的唯一性原则，影响了集群的稳定运行。
2. 单节点变更是利用“一次变更一个节点，不会同时存在旧配置和新配置`2`个‘大多数’”的特性，实现成员变更。
3. 因为联合共识实现起来复杂，不好实现，所以绝大多数 Raft 算法的实现，采用的都是单节点变更的方法（比如`Etcd`、`Hashicorp Raft`）。
   - 其中，`Hashicorp Raft`单节点变更的实现，是由`Raft`算法的作者迭戈·安加罗（`Diego Ongaro`）设计的，很有参考价值。