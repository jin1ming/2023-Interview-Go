# 高性能媒体服务 C++ 架构设计方案 (面试实战版)

> 场景：面试官询问“请介绍一下你设计的 C++ 媒体服务架构，以及你是如何解决高性能与扩展性问题的？”

## 开场白

面试官您好。在设计这套 C++ 媒体服务时，我的核心目标是构建一个**低延迟、高并发且易于扩展**的视频流处理系统。为了实现平均延迟低于 1ms 的指标，我从以下四个维度构建了整个系统架构：

---

### 1. 整体架构：基于 Reactor 的事件驱动模型

首先，为了追求极致的性能，我没有采用传统的多线程阻塞模型（Thread-per-Connection），而是设计了基于 **epoll 的 Reactor 模式**。

整个服务的核心是一个单线程的 **EventLoop (事件循环)**，它全权负责所有的 IO 事件，包括：
*   **网络 IO**：处理 gRPC/HTTP 客户端连接。
*   **定时任务**：管理心跳和超时。
*   **视频流处理**：这是最关键的部分，负责视频帧的接收与分发。

**设计收益**：这种单线程非阻塞的设计，最大限度地减少了线程上下文切换和锁的开销，是实现毫秒级低延迟的基石。

---

### 2. 设备接入层：插件式工厂模式

在接入层设计上，考虑到系统需要支持海康、大华、RTSP 等多种异构视频源，且未来肯定会有新增，我采用了一套 **"自注册的工厂模式"** 来实现解耦。

*   **接口抽象**：我定义了一个统一的 `Source` 抽象基类，屏蔽了不同厂商 SDK 的初始化、登录、拉流差异。
*   **自动注册**：利用 C++ 的静态全局变量初始化特性，我设计了一个自动注册机制。
    *   *实现细节*：每种相机的实现（如 `HikSource`）只需在自己的 `.cpp` 文件中定义一个静态注册对象，构造时自动将自己“挂载”到全局工厂中。
    
**设计收益**：这实现了完美的开闭原则（OCP）。主业务逻辑完全不感知具体相机型号，新增设备支持只需增加文件并链接，无需修改一行核心代码。

---

### 3. 核心难点：异步 EventLoop 与同步 SDK 的桥接

这是设计中最具挑战性的部分。大多数厂商 SDK（如海康）都在其内部线程中通过回调函数推送数据。如果我们直接在回调中处理数据，不仅会引入复杂的锁竞争，还可能阻塞 SDK 内部线程。

为了解决这个问题，我设计了 **"EventFD + 无锁队列"** 的桥接方案：

1.  **无锁队列 (Lock-Free Queue)**：
    当 SDK 线程收到视频帧时，仅做一件事：将数据指针塞入一个预先分配好的无锁队列。这个操作是原子的，极快。
    
    #### 选型思考：环形缓冲 (RingBuffer) vs 链表 (Linked List)
    这里我明确选择了**定长数组实现的环形缓冲区**，而非链表。尽管链表可以实现无界队列，但在高频视频流场景下有两大劣势：
    *   **内存碎片与开销**：链表节点需要频繁的 `new` 和 `delete`，对于每秒几十帧的视频流，这会带来巨大的内存分配器开销（Allocator Overhead）和碎片化。
    *   **缓存友好性差**：链表节点在堆内存中是离散的，容易导致 CPU Cache Miss。而数组内存连续，CPU 预取（Prefetching）效率极高。
    
2.  **EventFD 信号通知**：
    紧接着，SDK 线程向一个 `eventfd` 写入 8 字节数据，作为“有新数据”的信号。
    
3.  **主循环异步唤醒**：
    主线程的 EventLoop 监听着这个 `eventfd`。一旦可读，主线程立即醒来，从队列中批量取走视频帧进行后续分发。

**设计收益**：通过这种设计，我将多线程并发问题转化为了单线程的事件处理问题，彻底消除了核心链路上的互斥锁，保证了系统的吞吐量和稳定性。

---

### 4. 数据分发层：零拷贝、Pub-Sub 与写时复制

当一帧数据到达主线程后，可能同时有直播、录像、AI 分析等多个业务方需要消费。这是一个典型的 **Pub-Sub (发布-订阅)** 模型。

#### 4.1 核心分发逻辑
*   **Stream (流主题)**：维护所有订阅者（Sink）列表。
*   **Sink (消费者)**：对应具体的业务需求（如 WebSocket 推流）。

#### 4.2 写时复制 (Copy-On-Write, COW) 的应用
在订阅者列表的管理上，我引入了 **写时复制 (Copy-On-Write)** 技术来进一步降低锁竞争。

*   **场景**：在高并发场景下，客户端的连接（Connect）和断开（Disconnect）会频繁修改订阅者列表，而主线程的分发（Dispatch）又需要高频读取这个列表。
*   **传统做法**：使用读写锁（Read-Write Lock）。但分发频率极高（每秒几百次），即使是读锁也会带来 CPU 开销和潜在的写线程饥饿。
*   **COW 方案**：
    *   **读操作 (分发)**：直接读取当前的订阅者列表指针，无需加锁，性能极快。
    *   **写操作 (新增/删除订阅)**：
        1.  申请新内存，拷贝原列表。
        2.  在新列表上修改。
        3.  原子替换列表指针。
*   **收益**：虽然修改列表变重了，但鉴于“连接/断开”的频率远低于“帧分发”的频率，这种牺牲是完全值得的。它保证了核心的**帧分发路径是完全无锁 (Lock-Free) 的**。

#### 4.3 慢消费者隔离 (Flow Control)
如果某个客户端网络很差（慢消费者），不能让它阻塞主线程或其他正常的客户端。
*   **独立队列**：每个 Sink 内部维护一个独立的发送队列（如 `std::deque<Frame>`）。
*   **非阻塞写入**：`OnFrame` 操作仅仅是将 shared_ptr 放入 Sink 的队列中，立即返回。
*   **丢帧保护**：如果某个 Sink 的队列已满（达到高水位），新的帧会被直接丢弃（Drop），从而保护系统内存不被撑爆。

---

### 总结

综上所述，这套架构通过 **Reactor 模型** 确立了高性能基调，通过 **EventFD 桥接** 解决了异构 SDK 集成难题。特别是在分发层，结合 **零拷贝** 传输数据与 **写时复制 (COW)** 管理订阅列表，实现了核心链路的**全无锁化**。这套设计使得服务能够以极低的资源占用，跑满硬件的性能极限。
